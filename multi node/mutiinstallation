-In master
sudo passwd osboxes

-adduser
sudo adduser hduser
sudo visudo
-add this file
hduser  ALL=(ALL:ALL) NOPASSWD:ALL

-switch user to hduser
sudo apt update
sudo apt install openjdk-8-jdk -y
sudo apt install net-tools -y
sudo apt install ssh vim pdsh
cd

--set hostname
sudo hostnamectl set-hostname master
sudo hostname master
(open new terminal)

-open bashrc file
nano .bashrc

#JAVA
	export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/
	export JRE_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre

	#Hadoop Environment Variables
	export HADOOP_HOME=/usr/local/hadoop
	export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
	export HADOOP_LOG_DIR=$HADOOP_HOME/logs

	# Add Hadoop bin/ directory to PATH
	export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
	export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
	export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native" 
	export PDSH_RCMD_TYPE=ssh
--------
-reload bashrc file
source ~/.bashrc
--------
-setup ssh & enable
sudo systemctl start ssh
sudo systemctl enable ssh

-Enable passwordless SSH
ssh-keygen -t rsa
ssh-copy-id -i ~/.ssh/id_rsa.pub hduser@localhost 
ssh-copy-id -i ~/.ssh/id_rsa.pub hduser@master
ssh hduser@localhost
exit

-Download Hadoop
wget -c -O hadoop.tar.gz https://dlcdn.apache.org/hadoop/common/hadoop-3.2.4/hadoop-3.2.4.tar.gz
sudo mkdir /usr/local/hadoop
tar xvzf hadoop.tar.gz
sudo mv hadoop-3.2.4/* /usr/local/hadoop
sudo chown -R hduser:hduser /usr/local/hadoop
sudo chmod -R 755 /usr/local/hadoop

cd /usr/local/hadoop/etc/hadoop
ls

-----
nano core-site.xml

<configuration>
    	<property>
        <name>fs.defaultFS</name>
        <value>hdfs://master:9000</value>
    	</property>
	</configuration>
-----
nano  hadoop-env.sh
JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"
-----
nano hdfs-site.xml<configuration>
        <property>
        <name>dfs.name.dir</name>
        <value>/usr/local/hadoop/hd-data/nn</value>
        </property>
        <property>
        <name>dfs.replication</name>
        <value>2</value>
        </property>
        </configuration>
-----
nano mapred-site.xml
<configuration>
    	<property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
   	</property>
    	<property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
   	</property>
	</configuration>
-----
nano yarn-site.xml
<configuration>
    	<property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    	</property>
	</configuration>
-----
nano workers (clear localhost and enter dn1,dn2,...this is case sensitive)
-----

cd
sudo mkdir /usr/local/hadoop/hd-data
ip a
-note down ip
192.168.144.129 master(may vary)
sudo nano /etc/hosts

-enter ip in etc host file
192.168.144.129 master
sudo init 0/manually restart vm

-take snapshot
-make 2 clones for datanode

************************

-open datanode1 (first configuration)
-login as hduser

-change hostname
sudo hostnamectl set-hostname dn1
sudo hostname dn1
-open new terminal

cat .bashrc
cd /usr/local/hadoop/etc/hadoop
------
nano hdfs-site.xml

<configuration>
    	<property>
        <name>dfs.data.dir</name>
        <value>/usr/local/hadoop/hd-data/dn</value>
   	</property>
    	<property>
        <name>dfs.replication</name>
        <value>2</value>
    	</property>
	</configuration>
------
nano yarn-site.xml
<configuration>
<property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/usr/local/hadoop/hd-data/yarn/data</value>
    </property>
    <property>
        <name>yarn.nodemanager.logs-dirs</name>
        <value>/usr/local/hadoop/hd-data/yarn/logs</value>
    </property>
    <property>
        <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-perdisk-percentage</name>
        <value>99.9</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>
------
- nano workers
- localhost
------
cd
sudo mkdir /usr/local/hadoop/yarn
ip a (note ip addr)

sudo vim /etc/hosts
-add ip address in host file
192 master
192 dn1
192 dn2

*****************

-open datanode2 (second configuration)
-login as hduser

-change hostname
sudo hostnamectl set-hostname dn2
sudo hostname dn2
-open new terminal

cat .bashrc
cd /usr/local/hadoop/etc/hadoop
------
nano hdfs-site.xml

<configuration>
    	<property>
        <name>dfs.data.dir</name>
        <value>/usr/local/hadoop/hd-data/dn</value>
   	</property>
    	<property>
        <name>dfs.replication</name>
        <value>2</value>
    	</property>
	</configuration>
------
nano yarn-site.xml
<configuration>
<property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/usr/local/hadoop/hd-data/yarn/data</value>
    </property>
    <property>
        <name>yarn.nodemanager.logs-dirs</name>
        <value>/usr/local/hadoop/hd-data/yarn/logs</value>
    </property>
    <property>
        <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-perdisk-percentage</name>
        <value>99.9</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>
------
- nano workers
- localhost
------
cd
sudo mkdir /usr/local/hadoop/yarn
ip a (note ip addr)

sudo nano /etc/hosts
-add ip address in host file
192 master
192 dn1
192 dn2

**************************
-Open master
ip a
sudo nano /etc/hosts
enter ip of both data node
ssh-copy-id -i ~/.ssh/id_rsa.pub hduser@dn1
ssh-copy-id -i ~/.ssh/id_rsa.pub hduser@dn2
cd /usr/local/hadoop/etc/hadoop
nano workers
	(dn1,dn2)

hadoop namenode -format
-Restart all datanodes and master

-Now open master
start-all.sh
jps

-Now go in dn1,dn2 and do
jps
-----
In master
http://master:9870







